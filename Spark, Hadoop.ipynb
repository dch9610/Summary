{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 사용하기\n",
    "## vagrant 환경설정\n",
    "- CMD> vagrant --version\n",
    "- CMD> vagrant plugin install vagrant-disksize\n",
    "\n",
    "- //파일명 : Vagrantfile에 붙여넣기\n",
    "\n",
    "- Vagrant.configure(\"2\") do |config|\n",
    "  - config.vm.box = \"ubuntu/bionic64\"  #18.04\n",
    "  - config.vm.network \"forwarded_port\", guest: 3000, host: 3000\n",
    "  - config.vm.network \"forwarded_port\", guest: 9000, host: 9000    # hadoop\n",
    "  - config.vm.network \"forwarded_port\", guest: 9870, host: 9870    # hadoop ui\n",
    "  - config.vm.network \"forwarded_port\", guest: 9864, host: 9864    # hadoop\n",
    "  - config.vm.network \"forwarded_port\", guest: 9866, host: 9866    # hadoop ui \n",
    "  - config.vm.network \"forwarded_port\", guest: 4000, host: 4000\n",
    "  - config.vm.network \"forwarded_port\", guest: 8888, host: 8888    # jupyter notebook\n",
    "  - config.vm.network \"forwarded_port\", guest: 7077, host: 7077    # spark master\n",
    "  - config.vm.network \"forwarded_port\", guest: 2181, host: 2181    # zookeeper \n",
    "  - config.vm.network \"forwarded_port\", guest: 9092, host: 9092    # kafka\n",
    "  - config.vm.network \"forwarded_port\", guest: 4040, host: 4040    # spark ui\n",
    "  - config.vm.network \"forwarded_port\", guest: 27017, host: 27017  # mongodb\n",
    "\n",
    "  - config.vm.network \"forwarded_port\", guest: 5000, host: 5000    # flask\n",
    "  - config.vm.network \"forwarded_port\", guest: 8080, host: 8089    # oracle\n",
    "  - config.vm.network \"forwarded_port\", guest: 1521, host: 1522    # oracle\n",
    "\n",
    "## pyspark 설치\n",
    "- $ conda install pyspark=2.4.5\n",
    "\n",
    "\n",
    "## vagrant를 실행\n",
    "- jupyter notebook을 사용하기 위해 환경설정파일 생성\n",
    "    - config.vm.network \"forwarded_port\", guest: 4040, host: 4040  # 추가함\n",
    "    - 주피터 환경설정파일 생성 $ jupyter notebook --generate-config \n",
    "    \n",
    "- $ nano ~/.jupyter/jupyter_notebook_config.py 접속해서 수정\n",
    "    - 048라인 : c.NotebookApp.allow_origin = '*'  # 외부 접속 허용하기\n",
    "    - 204라인 : c.NotebookApp.ip = '10.0.2.15'  #vagrant 사용시  내부 ip로\n",
    "    - 272라인 : c.NotebookApp.open_browser = False # 시작 시 서버PC에서 주피터 노트북 창이 열릴 필요 없음\n",
    "    - 292라인 : c.NotebookApp.port = 8888   #포트 설정\n",
    "    \n",
    "- $ jupyter notebook : 주피터 노트북 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리\n",
    "- from pyspark.sql import SparkSession # 스파크 사용\n",
    "- from pyspark.sql.functions import expr #컬럼명 변경할 때\n",
    "- from pyspark.sql.functions import col #데이터 프레임 컬럼명 사용가능\n",
    "- from graphframes import GraphFrame #관계형 그래프 적용\n",
    "- from pyspark.sql.functions import desc    \n",
    "- import matplotlib.pyplot as plt # 그래프 출력\n",
    "- import matplotlib.font_manager as fm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 버전확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .enableHiveSupport().appName(\"hive02\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version # 2.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(1,'a',10),(2,'b',20),(3,'c',30),(4,'c',40)]).toDF(\"id\",\"name\",\"age\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하둡에 데이터 저장\n",
    "- hadoop hdfs에 /test1 폴더를 생성함.\n",
    "    - $ hdfs dfs -mkdir /test1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.mode(\"Overwrite\").option(\"header\", \"true\").csv(\"hdfs://127.0.0.1:9000/test1/df2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.option(\"header\", \"true\") \\\n",
    "     .csv('hdfs://127.0.0.1:9000/test1/df2')  \n",
    "\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 프레임을 판다스 구조로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# spark dataframe to pandas\n",
    "df4 = df3.toPandas()\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 판다스를 데이터 프레임 구조로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = spark.createDataFrame(df4)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop에 Hive 적용시키기\n",
    "- hive 폴더 생성\n",
    "    - $hdfs dfs -mkdir -p /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive 적용\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "        .enableHiveSupport().appName(\"hive01\") \\\n",
    "        .config(\"spark.datasource.hive.metastore.uris\",\"hdfs://127.0.0.1:9000\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\",\"hdfs://127.0.0.1:9000/user/hive/warehouse\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\",\"hive\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹데이터 저장하고 출력하는 방법\n",
    "- 1. 나의 컴퓨터에서 pyspark 폴더로 들어가 크롬드라이버를 이용해 자료 수집을 하는 파이썬을 만든다.\n",
    "- 2. 수집된 자료를 mongoDB에 넣는다 # mongoDB에 데이터 넣기전에 포트번호 확인\n",
    "    - conn = pymongo.MongoClient('127.0.0.1',27017) # 맥은 '192.168.99.100', 32888\n",
    "\t- db = conn.get_database(\"db1\") #없으면 db1생성\n",
    "\t- tb = db.get_collection(\"t01\") #collection 생성\n",
    "\t- tb.insert_many(df.to_dict('records')) \n",
    "    \n",
    "- 3. 4040포트의 환경으로 들어가기\n",
    "    - cd /home/vagrant/anaconda3/lib/python3.7/site-packages/pyspark/jars # 여기 경로에 밑에 2개 입력\n",
    "        * wget https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.11/2.4.1/mongo-spark-connector_2.11-2.4.1.jar\n",
    "        * wget https://repo1.maven.org/maven2/org/mongodb/mongo-java-driver/3.9.1/mongo-java-driver-3.9.1.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mongoDB에 저장된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"myApp\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1:27017/db1.t01\") \\  # 데이터 가져오기\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1:27017/db1.t02\") \\ # 데이터 저장\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.1') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mongoDB에서 읽기\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 관계형 그래프 사용\n",
    "- $ cd /home/vagrant/anaconda3/lib/python3.7/site-packages/pyspark/jars/ # 해당 경로로 이동\n",
    "    - wget http://dl.bintray.com/spark-packages/maven/graphframes/graphframes/0.7.0-spark2.4-s_2.11/graphframes-0.7.0-spark2.4-s_2.11.jar # 설치\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적용\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .enableHiveSupport().appName(\"spark_app1\") \\\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.7.0-spark2.4-s_2.11')\\ # 관계형 그래프 적용\n",
    "    .getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
